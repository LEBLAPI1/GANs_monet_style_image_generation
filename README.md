# GANs_monet_style_image_generation

# Week 5 GAN assignment:

# Generating Monet-Style Images Using GANs

## 1. Description of the Problem

### Problem Overview
### This project involves the creation of a GANs capable of generating images that mimic the style of Monet. By leveraging modern machine learning techniques such as downsampling and upsampling to automate the generation of artistic images that resemble the unique style of Monet's impressionist paintings. This notebook shows potential towards the creative art domain. 

### Data Description
### The dataset is divided into two parts: Monet paintings and real landscape photos. The Monet dataset includes various works by the artist, showcasing his iconic brushwork and color palette. The real photos dataset consists of landscape images, serving as the input for our GAN model. Both datasets are processed to a uniform size of 256x256 pixels, preparing them for the training process.

## 2. Exploratory Data Analysis (EDA)
### Visualization and Insights
### Using the display_images function, you can visually explore the 10 samples from the Monet paintings and 10 real photo examples. This exploration reveals the diversity in colors, subjects, and styles present in Monet's paintings compared to the more realistic hues and compositions of the landscape photos. This diversity is crucial for the GAN's learning process, as it must capture and replicate the distinct impressionist style of Monet's artwork.

## 3. Model Building and Training
### GAN Architecture
### The GAN consists of two main components: a generator and a discriminator. The generator network is tasked with transforming input landscape photos into outputs resembling Monet's paintings. Conversely, the discriminator network distinguishes between actual Monet paintings and the generated images. The architecture incorporates downsample and upsample layers, alongside dropout techniques, to facilitate this complex transformation process.

### Preprocessing
### The images are normalized to a [0, 1] scale to ensure consistency in model input. Additionally, images are resized to 256x256 pixels to standardize the dataset and streamline the training process.

### Training Process
### The model is trained using Adam optimizers with specific learning rates to optimize both the generator and discriminator networks. The training involves generator loss, discriminator loss, and cycle consistency loss functions, each playing a pivotal role in refining the generated images' quality. TensorFlow's @tf.function decorator is utilized to compile the training step into a graph, enhancing execution speed and efficiency.
### Please note that without the normalization step (Norm function) the learning process within the epochs does not converge.

## 4. Results
### Generated Images
### The GAN successfully generates images that capture the essence of Monet's style, as demonstrated by side-by-side comparisons with actual Monet paintings and input photos. These comparisons highlight the model's ability to replicate Monet's distinctive color palettes and textures.

### Loss Metrics
### Training loss plots for the generator and discriminator reveal how the model's performance evolved over epochs, showcasing gradual improvement and stabilization in generating Monet-style images.

### Discussion
### While the generated images showcase notable strengths, such as accurate color palettes and impressionist textures, areas for improvement remain, particularly in achieving Monet's unique brushstroke style. Patterns and anomalies in the generated images suggest areas for further refinement and optimization. One possible improvement here is to increase the size of the individual convolutional layers to capture larger brush strokes and another possibility is to add more layers within the network to learn more complex patterns. One can see from the generated images below there is a grid like pattern which is a sign it is generated by a machine instead of a person.

## 5. Conclusion and Discussion
### This project demonstrates the potential of GANs to generate convincing Monet-style images, marking a successful intersection of AI and art. Challenges encountered in training stability and model convergence were addressed through architectural adjustments and hyperparameter tuning (example adding the Norm layer function and increasing dropout). Insights gained emphasize the importance of dataset quality and architecture design in achieving high-quality generated images. I think more examples of the Monet images as only 300 image JPG files were provided as apposed to 7000+ images of real photos. Deep NNs require a lot of data to be able to learn the patterns correctly. Future work could explore different GAN architectures or incorporate additional datasets to enhance model robustness and output diversity.
